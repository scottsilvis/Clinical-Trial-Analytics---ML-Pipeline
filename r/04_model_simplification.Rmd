---
title: "04_model_simplification.Rmd"
output: html_notebook
---

## Objective 

The goals of this file is to investigate how much of the predictive ability of the model is due to baseline_severity, and whether a simpler, more defensible model can be made without relying on a composite clinical score. 

While baseline severity is a strong predictor of response, reliance on a composite clinical score may limit interpretability and deployability. In many real-world settings, simpler models using objective measurements are preferred. This phase evaluates how predictive performance changes as baseline_severity is removed or de-emphasized, and whether meaningful signal remains in other timepoint 0 features.

## Comparison Strategy

The following models will be evaluated using identical data splits and evaluation procedure:

- model_a <- Reference Model. Full baseline model
- model_b <- baseline model - baseline_severity
- model_c <- objective labs only (crp_mgL, alt_U_L, egfr_ml_min)
- model_d <- model_c + demographics (sex, age, bmi, smoker, comorbidity_count)

These models are selected specifically to look at trade offs between the predictive performance, interpretability, and simplicity rather than to identify a single best model. 

## Evaluation Criteria

Models are evaluated using the same train/test split and the same metrics as in the baseline analysis. Because response is rare, precisionâ€“recall AUC is the primary metric with ROC-AUC being used for context. Changes in performance are interpreted as tradeoffs rather than absolute improvements.

## Libraries Needed

```{r}
library(caret)
library(pROC)
library(PRROC)
library(RSQLite)
library(dplyr)
```

## Code needed from 02_baesline_EDA and 03_baseline_modeling

```{r}
conn <- dbConnect(SQLite(), "clinical_trial.db")

patients <- dbGetQuery(conn, "SELECT * FROM patients;")
# sites    <- dbGetQuery(conn, "SELECT * FROM sites;")
outcomes <- dbGetQuery(conn, "SELECT * FROM outcomes;")
# visits <- dbGetQuery(conn, "SELECT * FROM visits")

baseline_df <- patients %>%
  inner_join(
    outcomes %>% select(patient_id, responder_30pct),
    by = "patient_id"
  )

baseline_df <- baseline_df %>%
  select(
    responder_30pct,
    treatment_arm,
    sex,
    age,
    bmi,
    smoker,
    comorbidity_count,
    baseline_severity,
    crp_mgL,
    alt_U_L,
    egfr_ml_min,
    self_reported_adherence
  )

set.seed(123) 

train_idx <- createDataPartition(
  baseline_df$responder_30pct,
  p = 0.8,
  list = FALSE
)

train_df <- baseline_df[train_idx, ]
test_df  <- baseline_df[-train_idx, ]

train_df$sex <- factor(train_df$sex)
test_df$sex  <- factor(test_df$sex, 
                       levels = levels(train_df$sex))

train_df$treatment_arm <- factor(train_df$treatment_arm)
test_df$treatment_arm  <- factor(test_df$treatment_arm, 
                                 levels = levels(train_df$treatment_arm)
                                 )


```

```{r}
prop.table(table(baseline_df$responder_30pct))
prop.table(table(train_df$responder_30pct))
prop.table(table(test_df$responder_30pct))
```

I tested the distribution again just to confirm that everything looks right. Using the same seed should have ensured that I got the same datasets as in 03_baseline_modeling but it is good to check. 

## model_a

```{r}
model_a <- glm(formula = responder_30pct ~ 
               as.factor(sex) + 
               age + 
               bmi +
               smoker +
               comorbidity_count +
               baseline_severity +
               crp_mgL + 
               alt_U_L +
               egfr_ml_min +
               self_reported_adherence +
               treatment_arm, 
             family=binomial(link='logit'), 
             data=train_df)
summary(model_a)
```

```{r}
test_pred_prob <- predict(model_a, newdata = test_df, type = "response")
summary(test_pred_prob)
head(test_pred_prob)
```

```{r}
roc_obj <- roc(
  response = test_df$responder_30pct,
  predictor = test_pred_prob
)

auc(roc_obj)
```

```{r}
pr_obj <- pr.curve(
  scores.class0 = test_pred_prob[test_df$responder_30pct == 1],
  scores.class1 = test_pred_prob[test_df$responder_30pct == 0],
  curve = TRUE
)

pr_obj$auc.integral
```


## model_b

```{r}
# model_a - baseline_severity
model_b <- glm(formula = responder_30pct ~ 
               as.factor(sex) + 
               age + 
               bmi +
               smoker +
               comorbidity_count +
               crp_mgL + 
               alt_U_L +
               egfr_ml_min +
               self_reported_adherence +
               treatment_arm, 
             family=binomial(link='logit'), 
             data=train_df)
summary(model_b)
```

```{r}
test_pred_prob <- predict(model_b, newdata = test_df, type = "response")
summary(test_pred_prob)
head(test_pred_prob)
```

```{r}
roc_obj <- roc(
  response = test_df$responder_30pct,
  predictor = test_pred_prob
)

auc(roc_obj)
```

```{r}
pr_obj <- pr.curve(
  scores.class0 = test_pred_prob[test_df$responder_30pct == 1],
  scores.class1 = test_pred_prob[test_df$responder_30pct == 0],
  curve = TRUE
)

pr_obj$auc.integral
```


## model_c

```{r}
# objective labs only
model_c <- glm(formula = responder_30pct ~ 
               as.factor(sex) + 
               age + 
               bmi +
               smoker +
               comorbidity_count +
               crp_mgL + 
               alt_U_L +
               egfr_ml_min,
             family=binomial(link='logit'), 
             data=train_df)
summary(model_c)
```

```{r}
test_pred_prob <- predict(model_c, newdata = test_df, type = "response")
summary(test_pred_prob)
head(test_pred_prob)
```

```{r}
roc_obj <- roc(
  response = test_df$responder_30pct,
  predictor = test_pred_prob
)

auc(roc_obj)
```

```{r}
pr_obj <- pr.curve(
  scores.class0 = test_pred_prob[test_df$responder_30pct == 1],
  scores.class1 = test_pred_prob[test_df$responder_30pct == 0],
  curve = TRUE
)

pr_obj$auc.integral
```


## model_d

```{r}
# model_c + demographics
model_d <- glm(formula = responder_30pct ~ 
               as.factor(sex) + 
               age + 
               bmi +
               smoker +
               comorbidity_count +
               crp_mgL + 
               alt_U_L +
               egfr_ml_min +
               self_reported_adherence,
             family=binomial(link='logit'), 
             data=train_df)
summary(model_d)
```

```{r}
test_pred_prob <- predict(model_d, newdata = test_df, type = "response")
summary(test_pred_prob)
head(test_pred_prob)
```

```{r}
roc_obj <- roc(
  response = test_df$responder_30pct,
  predictor = test_pred_prob
)

auc(roc_obj)
```

```{r}
pr_obj <- pr.curve(
  scores.class0 = test_pred_prob[test_df$responder_30pct == 1],
  scores.class1 = test_pred_prob[test_df$responder_30pct == 0],
  curve = TRUE
)

pr_obj$auc.integral
```

Removing baseline_severity resulted in a substantial loss of predictive performance, with models relying on demographics, treatment assignment, and enrollment-time laboratory values performing near baseline prevalence. This indicates that baseline_severity captures clinically meaningful information that is not recoverable from individual demographic variables or timepoint-0 labs alone. While treatment assignment has a strong population-level effect, it does not meaningfully improve individual-level precision in the absence of baseline severity. These findings suggest that enrollment-time simplification is limited in this setting and motivate exploration of early on-treatment data as a potential source of additional actionable signal.