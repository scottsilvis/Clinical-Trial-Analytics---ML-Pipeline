---
title: "03_exploratory_data_analysis"
output: html_notebook
---

Targets for EDA:

The "patients" df holds all of the information that is available at the time of the study. I will begin my focus here to prevent any data leakage. Currently I am blind to what was found, and I want to maintain that for as long as possible. 

I have the following attributes to consider:

patient_id
site_id
enroll_date
sex
age
bmi
smoker
comorbidity_count
baseline_severity
crp_mgL
alt_U_L
egrf_ml_min
self_reported_adherence
treatment_arm

Of these attributes, everything is tied to the patient and not changeable by the organizers of the study, except treatment_arm. This could be argued in the case of site_id, but I will check that no patients visited multiple sites before I continue. 

Because everything is tied to patient ID, they are all covariates and will be handled later. What is most important at this stage, is that we did not introduce any bias when assigning treatment_arm. 


BASELINE EDA
```{r}
library(DBI)
library(RSQLite)
library(dplyr)
library(ggplot2)

conn <- dbConnect(SQLite(), "clinical_trial.db")
```

```{r}
patients <- dbGetQuery(conn, "SELECT * FROM patients;")
sites    <- dbGetQuery(conn, "SELECT * FROM sites;")
outcomes <- dbGetQuery(conn, "SELECT * FROM outcomes;")
visits <- dbGetQuery(conn, "SELECT * FROM visits")
```

MULTISITE TESTING
```{r}
patients_multi_site <- visits %>%
  group_by(patient_id) %>%
  summarise(n_sites = n_distinct(site_id)) %>%
  filter(n_sites > 1)

patients_multi_site
```


MEANS VS TREATMENT ARM
```{r}
means_vs_treatmentarm <- patients %>%
  group_by(treatment_arm) %>%
  summarise(
    n = n(),
    age_mean = mean(age, na.rm = TRUE),
    age_sd   = sd(age, na.rm = TRUE),
    bmi_mean = mean(bmi, na.rm = TRUE),
    bmi_sd   = sd(bmi, na.rm = TRUE),
    baseline_sev_mean = mean(baseline_severity, na.rm = TRUE),
    baseline_sev_sd   = sd(baseline_severity, na.rm = TRUE)
  )
```

```{r}
summary(aov(age ~ treatment_arm, data = patients))
summary(aov(bmi ~ treatment_arm, data = patients))
```

```{r}
table_sex <- table(patients$sex, patients$treatment_arm)
prop.table(table_sex, margin = 2)

chisq.test(table_sex)
```

No patients visited multiple sites, and an ANOVA did not find any significant differences between treatment_arm and age, or treatment_arm and bmi. There is some skew between sex and treatment_arm (χ² p = 0.03). The signal is weak, and we can treat this as a covariate to account for any bias. 

My next step is to define which baseline attributes have potential to be included in the model. 

**EXCLUDE**
patient_id - exclude (only functions as an identifier)

**CONSIDER LATER**
site_id - likely to be included later as an effect
enroll_date - may need to be included later as an effect

**INCLUDE**
sex 
age (measured)
bmi (measured)
smoker 
comorbidity_count
baseline_severity (clinical score, potentially less reliable than some other variables)
crp_mgL (measured)
alt_U_L (measured)
egfr_ml_min (measured)
self_reported_adherence (self reported. may be a correlate for engagement?)
treatment_arm (assigned by trial)

These variables are reasonable candidates for a baseline model because they are available at enrollment and may be related to outcome. Demographics and comorbidity help capture patient risk profiles. Baseline labs capture disease state objectively and baseline_severity captures disease state clinically. Treatment arm is included because it was assigned by the trial. I’ll evaluate models with and without treatment_arm to quantify how much predictive value it gives. Self-reported adherence is less reliable than measured variables, but it may still add signal and will be evaluated accordingly.

My next step is to build the baseline modeling dataframe

```{r}
baseline_df <- patients %>%
  inner_join(
    outcomes %>% select(patient_id, responder_30pct),
    by = "patient_id"
  )
```


```{r}
baseline_df <- baseline_df %>%
  select(
    responder_30pct,
    treatment_arm,
    sex,
    age,
    bmi,
    smoker,
    comorbidity_count,
    baseline_severity,
    crp_mgL,
    alt_U_L,
    egfr_ml_min,
    self_reported_adherence
  )
```

I have joined the responder_30pct column from the outcomes df to the patients df using the patient_id as the key. This has created a larger df named baseline_df that also contains attributes that I don't need such as sites and enroll_date. It also has patient_id, which I don't need anymore. I overwrite the baseline_df dropping these unnecessary attributes. I originally had 11 attributes I wanted to use in the baseline model, adding responder_30pct means I should have a df of 12 variables at this stage. I next need to check for any missing data. 

```{r}
sapply(baseline_df, function(x) mean(is.na(x)))
```

Three doesn't appear to be any missing data. Next is to look at the percentage of a successful outcome (as determined by responder_30pct).

```{r}
table(baseline_df$responder_30pct)
prop.table(table(baseline_df$responder_30pct))
```

I observed a severely imbalance response with only 3.1% of patients meeting the response criterion. This indicates a severely imbalanced classification problem. As a result, accuracy alone is not a meaningful evaluation metric, and subsequent modeling will focus on metrics appropriate for rare-event prediction.

Before fitting any models, I first assessed whether there is visible baseline signal associated with response. I examined the relationship between the binary outcome (responder_30pct) and a clinically meaningful baseline feature, baseline_severity. Because the responder_30pct variable is binomial, I will use a density plot, as it highlights the overlap between responders and non-responders. 


```{r}
ggplot(baseline_df, aes(
  x = baseline_severity, 
  fill = factor(responder_30pct)
  )
) +
  geom_density(alpha = 0.3) +
  labs(
    x = "Baseline Severity",
    y = "Density",
    fill = "Responder (30%)",
    title = "Baseline Severity by Response Status"
  )

```

Baseline severity shows clear separation between the two groups. Responders are concentrated primarily between baseline_severity values of ~20-60 and peaking near 37. The non-responders are shifted to the right, primarily between baseline severity values of ~35-80, with a peak near 57. While there is overlap, the magnitude and extent of separation suggest a meaningful baseline signal. This indicates that predictive modeling is likely feasible and that baseline severity is an important indicator. 

These findings indicate that meaningful baseline signal exists and that predictive modeling is feasible. The next phase focuses on establishing a baseline predictive model, which will continue in 03_baseline_modeling.

